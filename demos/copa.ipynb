{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**: demonstrates that the zero-shot text classification method [described here](https://stats.stackexchange.com/q/601159/337906) works ok on the [COPA task](https://people.ict.usc.edu/~gordon/copa.html). It's one of the [SuperGLUE tasks](https://super.gluebenchmark.com/tasks) in which labels have multiple tokens, in some sense.\n",
    "\n",
    "**Estimated run time**: ~1 min.\n",
    "\n",
    "**Environment**: See the [Setup section in the README](https://github.com/kddubey/lm-classification/#setup).\n",
    "\n",
    "**Other**: You have to have an OpenAI API key stored in the environment variable `OPENAI_API_KEY`. [Sign up here](https://openai.com/api/). This notebook will warn you about cost before incurring any. Running the whole notebook will cost ya north of <span>$</span>2!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Load data](#load-data)\n",
    "\n",
    "[Write prompt](#write-prompt)\n",
    "\n",
    "[Run model](#run-model)\n",
    "\n",
    "[Score](#score)\n",
    "\n",
    "[Evaluate CVS](#evaluate-cvs)\n",
    "\n",
    "[Evaluate question](#evaluate-question)\n",
    "\n",
    "[Evaluate single-token](#evaluate-single-token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import logging\n",
    "import sys\n",
    "from typing import Literal, Sequence\n",
    "\n",
    "import datasets as nlp_datasets\n",
    "import pandas as pd\n",
    "\n",
    "from lm_classification import classify\n",
    "from lm_classification.utils import api\n",
    "from utils import display_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## When hitting the OpenAI endpoints, we'll log any server errors\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    handlers=[logging.StreamHandler(stream=sys.stdout)],\n",
    "                    format='%(asctime)s :: %(name)s :: %(levelname)s :: '\n",
    "                           '%(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this MVP, let's evaluate on the [Choice of Plausible Alternatives (COPA) task](https://people.ict.usc.edu/~gordon/copa.html). I picked this first b/c I read it has multi-token labels, in some sense. It also looks cool. The [SuperGLUE tasks](https://super.gluebenchmark.com/tasks) in general are pretty cool.\n",
    "\n",
    "The classification problem is to pick 1 of 2 alternatives which caused or resulted in the premise. Here are two example pulled from the website:\n",
    "\n",
    "Example 1\n",
    "\n",
    "> Premise: The man broke his toe. What was the CAUSE of this?\n",
    ">\n",
    "> Alternative 1: He got a hole in his sock.\n",
    ">\n",
    "> Alternative 2: He dropped a hammer on his foot.\n",
    "\n",
    "\n",
    "Example 2\n",
    "\n",
    "> Premise: I tipped the bottle. What happened as a RESULT?\n",
    ">\n",
    "> Alternative 1: The liquid in the bottle froze.\n",
    ">\n",
    "> Alternative 2: The liquid in the bottle poured out.\n",
    "\n",
    "A classifier should predict Alternative 2 for Example 1, and Alternative 2 for Example 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set labels are hidden, so I'll score this zero-shot classifier on the train and validation sets. I didn't tune much of anything on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-28 02:30:57,931 :: datasets.builder :: WARNING :: Found cached dataset super_glue (C:/Users/kushd/.cache/huggingface/datasets/super_glue/copa/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n",
      "2023-02-28 02:31:03,176 :: datasets.builder :: WARNING :: Found cached dataset super_glue (C:/Users/kushd/.cache/huggingface/datasets/super_glue/copa/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    }
   ],
   "source": [
    "def load_super_glue(task_id: str, split: str):\n",
    "    return pd.DataFrame(nlp_datasets\n",
    "                        .load_dataset('super_glue', task_id, split=split))\n",
    "\n",
    "\n",
    "df = (pd.concat((load_super_glue('copa', 'train'),\n",
    "                 load_super_glue('copa', 'validation')))\n",
    "      .reset_index(drop=True)) ## the idx column is only unique w/in splits! fuhgetaboutit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>choice1</th>\n",
       "      <th>choice2</th>\n",
       "      <th>question</th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My body cast a shadow over the grass.</td>\n",
       "      <td>The sun was rising.</td>\n",
       "      <td>The grass was cut.</td>\n",
       "      <td>cause</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The woman tolerated her friend's difficult beh...</td>\n",
       "      <td>The woman knew her friend was going through a ...</td>\n",
       "      <td>The woman felt that her friend took advantage ...</td>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The women met for coffee.</td>\n",
       "      <td>The cafe reopened in a new location.</td>\n",
       "      <td>They wanted to catch up with each other.</td>\n",
       "      <td>cause</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The runner wore shorts.</td>\n",
       "      <td>The forecast predicted high temperatures.</td>\n",
       "      <td>She planned to run along the beach.</td>\n",
       "      <td>cause</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The guests of the party hid behind the couch.</td>\n",
       "      <td>It was a surprise party.</td>\n",
       "      <td>It was a birthday party.</td>\n",
       "      <td>cause</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0              My body cast a shadow over the grass.   \n",
       "1  The woman tolerated her friend's difficult beh...   \n",
       "2                          The women met for coffee.   \n",
       "3                            The runner wore shorts.   \n",
       "4      The guests of the party hid behind the couch.   \n",
       "\n",
       "                                             choice1  \\\n",
       "0                                The sun was rising.   \n",
       "1  The woman knew her friend was going through a ...   \n",
       "2               The cafe reopened in a new location.   \n",
       "3          The forecast predicted high temperatures.   \n",
       "4                           It was a surprise party.   \n",
       "\n",
       "                                             choice2 question  idx  label  \n",
       "0                                 The grass was cut.    cause    0      0  \n",
       "1  The woman felt that her friend took advantage ...    cause    1      0  \n",
       "2           They wanted to catch up with each other.    cause    2      1  \n",
       "3                She planned to run along the beach.    cause    3      0  \n",
       "4                           It was a birthday party.    cause    4      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to model COPA is to prompt an LM with (for Example 1):\n",
    "\n",
    "```\n",
    "The man broke his toe. This happened because \n",
    "```\n",
    "\n",
    "and use the LM to estimate the probabilities of the 2 alternatives conditional on this prompt. (See the **Example** section [here](https://stats.stackexchange.com/q/601159/337906) for a full description of what \"compute the probabilities\" actually means. Or, check out the code in [`lm_classification.classify.predict_proba_examples`](https://github.com/kddubey/lm-classification/blob/main/lm_classification/classify.py#L270).)\n",
    "\n",
    "A potential issue with this approach is that any 2 alternatives are gonna have really low probabilities. As a result, discriminating between alternatives may be, numerically and statistically, a bad idea. But hey, that's why this notebook is here: let's see if this issue really does tank accuracy. And even if it does, we could always provide the alternatives in the prompt, as would be done w/ a sampling approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(premise: str, question: Literal['cause', 'effect']):\n",
    "    if question == 'cause':\n",
    "        preface = 'This happened because' ## spaces are added when constructing Examples\n",
    "    elif question == 'effect':\n",
    "        preface = 'As a result,'\n",
    "    else:\n",
    "        raise ValueError( \"question must be 'cause' or 'effect'. Got \"\n",
    "                         f'{question}.')\n",
    "    return f'{premise} {preface}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prompt'] = [prompt(premise, question)\n",
    "                for premise, question\n",
    "                in zip(df['premise'], df['question'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ade61_row0_col0, #T_ade61_row0_col1, #T_ade61_row0_col2, #T_ade61_row0_col3, #T_ade61_row1_col0, #T_ade61_row1_col1, #T_ade61_row1_col2, #T_ade61_row1_col3, #T_ade61_row2_col0, #T_ade61_row2_col1, #T_ade61_row2_col2, #T_ade61_row2_col3 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ade61\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ade61_level0_col0\" class=\"col_heading level0 col0\" >prompt</th>\n",
       "      <th id=\"T_ade61_level0_col1\" class=\"col_heading level0 col1\" >choice1</th>\n",
       "      <th id=\"T_ade61_level0_col2\" class=\"col_heading level0 col2\" >choice2</th>\n",
       "      <th id=\"T_ade61_level0_col3\" class=\"col_heading level0 col3\" >label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ade61_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ade61_row0_col0\" class=\"data row0 col0\" >My body cast a shadow over the grass. This happened because</td>\n",
       "      <td id=\"T_ade61_row0_col1\" class=\"data row0 col1\" >The sun was rising.</td>\n",
       "      <td id=\"T_ade61_row0_col2\" class=\"data row0 col2\" >The grass was cut.</td>\n",
       "      <td id=\"T_ade61_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ade61_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_ade61_row1_col0\" class=\"data row1 col0\" >The woman tolerated her friend's difficult behavior. This happened because</td>\n",
       "      <td id=\"T_ade61_row1_col1\" class=\"data row1 col1\" >The woman knew her friend was going through a hard time.</td>\n",
       "      <td id=\"T_ade61_row1_col2\" class=\"data row1 col2\" >The woman felt that her friend took advantage of her kindness.</td>\n",
       "      <td id=\"T_ade61_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ade61_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_ade61_row2_col0\" class=\"data row2 col0\" >The women met for coffee. This happened because</td>\n",
       "      <td id=\"T_ade61_row2_col1\" class=\"data row2 col1\" >The cafe reopened in a new location.</td>\n",
       "      <td id=\"T_ade61_row2_col2\" class=\"data row2 col2\" >They wanted to catch up with each other.</td>\n",
       "      <td id=\"T_ade61_row2_col3\" class=\"data row2 col3\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x25b32740d60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_df(df, columns=['prompt', 'choice1', 'choice2', 'label'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we need to lowercase the choices. We could also strip the period, but I wanna see what happens if it's included. Ideally, minor but consistent things like that aren't an issue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for many SuperGLUE datasets, including COPA, the probability distribution over classes (alternative 1, 2 for COPA) is uniform. So we'll use `prior=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [classify.Example(prompt=record['prompt'],\n",
    "                             completions=(record['choice1'].lower(),\n",
    "                                          record['choice2'].lower()),\n",
    "                             prior=None)\n",
    "            for record in df.to_dict('records')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e3346fb1834338875aca01ccdcfd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 500 examples * 2 classes = 1000 OpenAI API requests\n",
    "## $0.35\n",
    "pred_probs = classify.predict_proba_examples(examples,\n",
    "                                             model='text-davinci-003',\n",
    "                                             ask_if_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For COPA, the scoring metric is accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_probs.argmax(axis=1) == df['label']).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put this number in context, we'll evaluate zero-shot classification via sampling (CVS) on `text-davinci-003`.\n",
    "\n",
    "But first, let's see how zero-shot curie performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6518e26aeac645e8a10a95f9c5282f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## $0.04\n",
    "pred_probs_curie = classify.predict_proba_examples(examples,\n",
    "                                                   model='text-curie-001',\n",
    "                                                   ask_if_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_probs_curie.argmax(axis=1) == df['label']).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate CVS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COPA isn't a great demo for this approach b/c there's a trivial way to transform multi-token labels to single tokens: just point to each choice with a single letter!\n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "The man broke his toe. This happened because\n",
    "A. He got a hole in his sock.\n",
    "B. He dropped a hammer on his foot.\n",
    "Answer A or B.\n",
    "```\n",
    "\n",
    "This prompt is a multiple choice question. And it could probably work well for all of the SuperGLUE tasks, because they're all binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b8f66_row0_col0, #T_b8f66_row0_col1, #T_b8f66_row1_col0, #T_b8f66_row1_col1, #T_b8f66_row2_col0, #T_b8f66_row2_col1 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b8f66\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b8f66_level0_col0\" class=\"col_heading level0 col0\" >prompt_mc</th>\n",
       "      <th id=\"T_b8f66_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b8f66_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b8f66_row0_col0\" class=\"data row0 col0\" >My body cast a shadow over the grass. This happened because\n",
       "A. The sun was rising.\n",
       "B. The grass was cut.\n",
       "Answer A or B.</td>\n",
       "      <td id=\"T_b8f66_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b8f66_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b8f66_row1_col0\" class=\"data row1 col0\" >The woman tolerated her friend's difficult behavior. This happened because\n",
       "A. The woman knew her friend was going through a hard time.\n",
       "B. The woman felt that her friend took advantage of her kindness.\n",
       "Answer A or B.</td>\n",
       "      <td id=\"T_b8f66_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b8f66_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b8f66_row2_col0\" class=\"data row2 col0\" >The women met for coffee. This happened because\n",
       "A. The cafe reopened in a new location.\n",
       "B. They wanted to catch up with each other.\n",
       "Answer A or B.</td>\n",
       "      <td id=\"T_b8f66_row2_col1\" class=\"data row2 col1\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x25b33f7a670>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prompt_mc(premise: str, question: Literal['cause', 'effect'],\n",
    "              choice1: str, choice2: str):\n",
    "    if question == 'cause':\n",
    "        preface = 'This happened because'\n",
    "    elif question == 'effect':\n",
    "        preface = 'As a result'\n",
    "    else:\n",
    "        raise ValueError( \"question must be 'cause' or 'effect'. Got \"\n",
    "                         f'{question}.')\n",
    "    return (f'{premise} {preface}\\n'\n",
    "            f'A. {choice1}\\n'\n",
    "            f'B. {choice2}\\n'\n",
    "             'Answer A or B.')\n",
    "\n",
    "\n",
    "df['prompt_mc'] = [prompt_mc(record['premise'], record['question'],\n",
    "                             record['choice1'], record['choice2'])\n",
    "                   for record in df.to_dict('records')]\n",
    "\n",
    "\n",
    "display_df(df, columns=['prompt_mc', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7931b661e249bca43bb65e18a35a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing probs:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## $0.41\n",
    "choices = api.gpt3_complete(df['prompt_mc'],\n",
    "                            ask_if_ok=True,\n",
    "                            model='text-davinci-003',\n",
    "                            max_tokens=5,\n",
    "                            logprobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions_mc = [choice['text'] for choice in choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_completion(completion: str, class_chars: Sequence[str],\n",
    "                       strip_chars: str=' \\n', default=-1) -> int:\n",
    "    if any(len(class_char) != 1 for class_char in class_chars):\n",
    "        raise ValueError('Elements of class_chars must be a single character.')\n",
    "    completion_stripped = completion.strip(strip_chars)\n",
    "    if not completion_stripped:\n",
    "        return default\n",
    "    completion_char_lower = completion_stripped[0].lower()\n",
    "    class_chars_lower = [class_char.lower() for class_char in class_chars]\n",
    "    try:\n",
    "        return class_chars_lower.index(completion_char_lower)\n",
    "    except ValueError:\n",
    "        return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_chars = ('A', 'B')\n",
    "pred_classes_cvs = [process_completion(completion, class_chars)\n",
    "                    for completion in completions_mc]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all of the sampled completions could be mapped to a label 0 or 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (pd.Series(pred_classes_cvs) != -1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.962"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_classes_cvs == df['label']).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performance boost seems pretty significant, which is a bit of a bummer. It should also be a bit cheaper, so there isn't a good reason to use this package instead of CVS."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how CVS w/ `text-curie-001` performs. Hypothesis: won't work well b/c I don't think this model was trained w/ human feedback, which makes the prompt unfamiliar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f56e7e10ca84812a0b005091d28cec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing probs:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## $0.04\n",
    "choices_curie = api.gpt3_complete(df['prompt_mc'],\n",
    "                                  ask_if_ok=True,\n",
    "                                  model='text-curie-001',\n",
    "                                  max_tokens=5,\n",
    "                                  logprobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions_mc_curie = [choice['text'] for choice in choices_curie]\n",
    "pred_classes_cvs_curie = [process_completion(completion, class_chars)\n",
    "                          for completion in completions_mc_curie]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many of these sampled completions are actually \"valid\", i.e., in the label set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_classes_cvs_curie = pd.Series(pred_classes_cvs_curie, index=df.index)\n",
    "(pred_classes_cvs_curie != -1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.282"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_classes_cvs_curie == df['label']).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, much worse than random guessing. Let's see how often the valid completions are accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5320754716981132"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mask_valid = pred_classes_cvs_curie != -1\n",
    "(pred_classes_cvs_curie[_mask_valid]\n",
    " ==\n",
    " df.loc[_mask_valid, 'label']).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to format a prompt-completion problem. Let's see how performance changes by formatting the problem as a question:\n",
    "\n",
    "```\n",
    "The man broke his toe. What was the cause of this? \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e628b_row0_col0, #T_e628b_row0_col1, #T_e628b_row0_col2, #T_e628b_row0_col3, #T_e628b_row1_col0, #T_e628b_row1_col1, #T_e628b_row1_col2, #T_e628b_row1_col3, #T_e628b_row2_col0, #T_e628b_row2_col1, #T_e628b_row2_col2, #T_e628b_row2_col3 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e628b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e628b_level0_col0\" class=\"col_heading level0 col0\" >prompt_question</th>\n",
       "      <th id=\"T_e628b_level0_col1\" class=\"col_heading level0 col1\" >choice1</th>\n",
       "      <th id=\"T_e628b_level0_col2\" class=\"col_heading level0 col2\" >choice2</th>\n",
       "      <th id=\"T_e628b_level0_col3\" class=\"col_heading level0 col3\" >label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e628b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e628b_row0_col0\" class=\"data row0 col0\" >My body cast a shadow over the grass. What was the cause of this?</td>\n",
       "      <td id=\"T_e628b_row0_col1\" class=\"data row0 col1\" >The sun was rising.</td>\n",
       "      <td id=\"T_e628b_row0_col2\" class=\"data row0 col2\" >The grass was cut.</td>\n",
       "      <td id=\"T_e628b_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e628b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e628b_row1_col0\" class=\"data row1 col0\" >The woman tolerated her friend's difficult behavior. What was the cause of this?</td>\n",
       "      <td id=\"T_e628b_row1_col1\" class=\"data row1 col1\" >The woman knew her friend was going through a hard time.</td>\n",
       "      <td id=\"T_e628b_row1_col2\" class=\"data row1 col2\" >The woman felt that her friend took advantage of her kindness.</td>\n",
       "      <td id=\"T_e628b_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e628b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e628b_row2_col0\" class=\"data row2 col0\" >The women met for coffee. What was the cause of this?</td>\n",
       "      <td id=\"T_e628b_row2_col1\" class=\"data row2 col1\" >The cafe reopened in a new location.</td>\n",
       "      <td id=\"T_e628b_row2_col2\" class=\"data row2 col2\" >They wanted to catch up with each other.</td>\n",
       "      <td id=\"T_e628b_row2_col3\" class=\"data row2 col3\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x25b352fad00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prompt_question(premise: str, question: Literal['cause', 'effect']):\n",
    "    if question == 'cause':\n",
    "        question_ = 'What was the cause of this?'\n",
    "    elif question == 'effect':\n",
    "        question_ = 'What happened as a result?'\n",
    "    else:\n",
    "        raise ValueError( \"question must be 'cause' or 'effect'. Got \"\n",
    "                         f'{question}.')\n",
    "    return f'{premise} {question_}'\n",
    "\n",
    "\n",
    "df['prompt_question'] = [prompt_question(premise, question)\n",
    "                         for premise, question\n",
    "                         in zip(df['premise'], df['question'])]\n",
    "\n",
    "\n",
    "display_df(df, columns=['prompt_question', 'choice1', 'choice2', 'label'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stay in-line with the way `text-davinci-003` was trained (with human feedback), let's use the correct separator between prompts and completions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <|endoftext|>\\n\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify.end_of_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_question = [classify.Example(prompt=record['prompt_question'],\n",
    "                                      completions=(record['choice1'],\n",
    "                                                   record['choice2']),\n",
    "                                      prior=None,\n",
    "                                      end_of_prompt=classify.end_of_text)\n",
    "                     for record in df.to_dict('records')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b6c45ffce9402e8acae3a3a12f354c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 500 examples * 2 classes = 1000 OpenAI API requests\n",
    "## $0.49\n",
    "pred_probs_question = classify.predict_proba_examples(examples_question,\n",
    "                                                      model='text-davinci-003',\n",
    "                                                      ask_if_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.854"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_probs_question.argmax(axis=1) == df['label']).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate single-token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the single-token transformation performs for COPA. My hypothesis is that it'll perform similarly to the multi-token approach. It could perform better simply b/c there's no weird aggregation of probabilities. I wouldn't be bummed if it performed better. B/c if I could control the backend, there's still a usability and computational benefit to the idea of returning probabilities for A and B instead of sampling from all possible tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_mc = [classify.Example(prompt=record['prompt_mc'],\n",
    "                                completions=('A', 'B'),\n",
    "                                prior=None,\n",
    "                                end_of_prompt=classify.end_of_text)\n",
    "               for record in df.to_dict('records')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5725812c5c1d400a978e3abe91a69bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 500 examples * 2 classes = 1000 OpenAI API requests\n",
    "## $0.81\n",
    "pred_probs_mc = classify.predict_proba_examples(examples_mc,\n",
    "                                                model='text-davinci-003',\n",
    "                                                ask_if_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_probs_mc.argmax(axis=1) == df['label']).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: performs slightly better than the multi-token approach. But it's worse than [CVS](#evaluate-cvs), which is difficult to explain. I should understand more about how sampling works. Ideally, a single `model()` call will get us all the data we need, and there's no need to sample."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68daa88f78f5c448099edb3a6d3dee27486a6add8824ae1cbe4c903ef8faec70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
