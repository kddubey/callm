{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Irx3LNbOKxNe"
      },
      "source": [
        "This notebook runs a tiny demo of a [GPTQd StableLM\n",
        "3B](https://huggingface.co/ethzanalytics/stablelm-tuned-alpha-3b-gptq-4bit-128g) on a\n",
        "classification task using CAPPr and then via sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SyatdGp441b5"
      },
      "outputs": [],
      "source": [
        "# check correct CUDA version\n",
        "import torch\n",
        "\n",
        "_cuda_version = torch.version.cuda\n",
        "_msg = (\n",
        "    \"Change the pip install auto-gptq command to the one for \"\n",
        "    f\"{_cuda_version} based on the list here: \"\n",
        "    \"https://github.com/PanQiWei/AutoGPTQ#quick-installation\"\n",
        ")\n",
        "\n",
        "assert _cuda_version == \"11.8\", _msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc41TGJm41b7"
      },
      "source": [
        "I'm gonna install `cappr` from source b/c sometimes I use this notebook to statistically gut check code changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uowdtw93h-UP"
      },
      "outputs": [],
      "source": [
        "!python -m pip install \"cappr[demos] @ git+https://github.com/kddubey/cappr.git\" \\\n",
        "git+https://github.com/huggingface/transformers \\\n",
        "auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ \\\n",
        "optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN3VQU-jEaXK"
      },
      "source": [
        "Uncomment and run this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NYmi2CkN94dF"
      },
      "outputs": [],
      "source": [
        "# !git lfs install\n",
        "# !git clone https://huggingface.co/ethzanalytics/stablelm-tuned-alpha-3b-gptq-4bit-128g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KpVgZ4L-bON",
        "outputId": "8ed75444-b483-49bd-e5c4-c42a6fdd7ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.json\t\t\t  README.md\n",
            "generation_config.json\t\t  special_tokens_map.json\n",
            "gptq_model-4bit-128g.safetensors  tokenizer_config.json\n",
            "quantize_config.json\t\t  tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "!ls stablelm-tuned-alpha-3b-gptq-4bit-128g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "caXw1gfuh-Ub"
      },
      "outputs": [],
      "source": [
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "import torch\n",
        "from transformers import AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "from cappr.huggingface import classify as fast\n",
        "from cappr.huggingface import classify_no_cache as slow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TPCBVanih-Ue"
      },
      "outputs": [],
      "source": [
        "_msg = (\n",
        "    \"This notebook must run on a GPU. A T4 instance is sufficient for the models \"\n",
        "    \"tested here.\"\n",
        ")\n",
        "assert torch.cuda.is_available(), _msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P1xACRa5wTR",
        "outputId": "e3efb13d-bb91-47b7-bc6b-801856df8242"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.utils.modeling:The safetensors archive passed at stablelm-tuned-alpha-3b-gptq-4bit-128g/gptq_model-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
            "WARNING:auto_gptq.modeling._base:GPTNeoXGPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.\n",
            "WARNING:auto_gptq.modeling._base:GPTNeoXGPTQForCausalLM hasn't fused mlp module yet, will skip inject fused mlp.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
          ]
        }
      ],
      "source": [
        "quantized_model_dir = \"stablelm-tuned-alpha-3b-gptq-4bit-128g\"\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    quantized_model_dir, use_triton=False, use_safetensors=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"StabilityAI/stablelm-tuned-alpha-7b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "o-IOK7Ijh-Uj"
      },
      "outputs": [],
      "source": [
        "# warm up model\n",
        "_ = model(**tokenizer([\"warm up\"], return_tensors=\"pt\").to(model.device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ibcahez-UWI"
      },
      "source": [
        "Chat format is pulled from [the HF page](https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MR0oLX7x6pXB"
      },
      "outputs": [],
      "source": [
        "stablelm_chat_template = \"\"\"\n",
        "<|SYSTEM|># {system_prompt}\n",
        "<|USER|>{user_message}<|ASSISTANT|>\n",
        "\"\"\".strip(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bQ0u3Z1hwA3x"
      },
      "outputs": [],
      "source": [
        "# Define a classification task\n",
        "feedback_types = (\n",
        "    \"the product is too expensive\",\n",
        "    \"the product uses low quality materials\",\n",
        "    \"the product is difficult to use\",\n",
        "    \"the product is great\",\n",
        ")\n",
        "\n",
        "\n",
        "# Write a prompt\n",
        "def prompt_func(product_review: str) -> str:\n",
        "    system_prompt = \"You are an expert at summarizing product reviews.\"\n",
        "    user_message = f\"This product review: {product_review}\\n\" \"is best summarized as:\"\n",
        "    return stablelm_chat_template.format(\n",
        "        system_prompt=system_prompt, user_message=user_message\n",
        "    )\n",
        "\n",
        "\n",
        "# Supply the texts you wanna classify\n",
        "product_reviews = [\n",
        "    \"I can't figure out how to integrate it into my setup.\",\n",
        "    \"Yeah it's pricey, but it's definitely worth it.\",\n",
        "]\n",
        "prompts = [prompt_func(product_review) for product_review in product_reviews]\n",
        "completions = feedback_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyhWG95E7265",
        "outputId": "dcd5fe63-7491-4a84-c9ee-5071899eb7e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|SYSTEM|># You are an expert at summarizing product reviews.\n",
            "<|USER|>This product review: I can't figure out how to integrate it into my setup.\n",
            "is best summarized as:<|ASSISTANT|>\n"
          ]
        }
      ],
      "source": [
        "print(prompts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Tk0aS7rch-Uo"
      },
      "outputs": [],
      "source": [
        "pred_probs_fast = fast.predict_proba(\n",
        "    prompts, completions, model_and_tokenizer=(model, tokenizer)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GHMCmfLJh-Up"
      },
      "outputs": [],
      "source": [
        "pred_probs_slow = slow.predict_proba(\n",
        "    prompts, completions, model_and_tokenizer=(model, tokenizer)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebGSjG0Qh-Uq",
        "outputId": "1c68c553-545c-42e1-d9b7-ac9ef6ceb4c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.278, 0.026, 0.438, 0.258],\n",
              "       [0.348, 0.073, 0.206, 0.373]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_probs_fast.round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm2ww0tWuu7E",
        "outputId": "f4617295-422a-449a-af4f-5bc585c49516"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.273, 0.024, 0.431, 0.271],\n",
              "       [0.347, 0.072, 0.199, 0.383]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_probs_slow.round(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44MfT3qZAbkW"
      },
      "source": [
        "Differences in the `fast` and `slow` module probabilities are not as small as I was\n",
        "hoping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHc45z2w_t_h",
        "outputId": "fe20966c-563e-48c2-f2f9-575dfcc3f229"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"I can't figure out how to integrate it into my setup.\",\n",
              " \"Yeah it's pricey, but it's definitely worth it.\"]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "product_reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG-Iqy7i_lhJ",
        "outputId": "747d3c4e-6e8c-4d78-ff10-55c3f6a4e044"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('the product is too expensive',\n",
              " 'the product uses low quality materials',\n",
              " 'the product is difficult to use',\n",
              " 'the product is great')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feedback_types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dWZuj68KXLD"
      },
      "source": [
        "Both predictions are correct. In the first product review, the 3rd class' probability is\n",
        "the highest. In the second product review, the 4th class' probability is highest\n",
        "(followed closely by the first class). Pretty impressive for an aggressively quantized\n",
        "3B parameter model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TODO: study `end_of_prompt` more closely. Or consider using a non-chat model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IqIZeV6CRT7"
      },
      "source": [
        "The baseline to beat is sampling from the LM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z-uEDnYAORy",
        "outputId": "2b37e938-143c-4ee2-db21-c0b297accc1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model 'GPTNeoXGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=100,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    batch_size=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBAMISBfESG2"
      },
      "source": [
        "Doubt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DIzn2Oa58uw",
        "outputId": "f70821ae-9b8f-4c98-b86d-ec52c807f663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|SYSTEM|># You are an expert at categorizing product reviews. Respond with the letter corresponding to the category which the given product review belongs to.\n",
            "<|USER|>Every product review belongs to one of these lettered categories:\n",
            "A. the product is too expensive\n",
            "B. the product uses low quality materials\n",
            "C. the product is difficult to use\n",
            "D. the product is great\n",
            "\n",
            "Which category does the following product review belong to?\n",
            "Product review: \"I can't figure out how to integrate it into my setup.\"\n",
            "\n",
            "Respond only with the letter choice: A or B or C or D.<|ASSISTANT|>\n"
          ]
        }
      ],
      "source": [
        "user_message = \"\"\"\n",
        "Every product review belongs to one of these lettered categories:\n",
        "A. the product is too expensive\n",
        "B. the product uses low quality materials\n",
        "C. the product is difficult to use\n",
        "D. the product is great\n",
        "\n",
        "Which category does the following product review belong to?\n",
        "Product review: \"I can't figure out how to integrate it into my setup.\"\n",
        "\n",
        "Respond only with the letter choice: A or B or C or D.\n",
        "\"\"\".strip(\"\\n\")\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are an expert at categorizing product reviews. \"\n",
        "    \"Respond with the letter corresponding to the category which the given \"\n",
        "    \"product review belongs to.\"\n",
        ")\n",
        "\n",
        "prompt = stablelm_chat_template.format(\n",
        "    system_prompt=system_prompt, user_message=user_message\n",
        ")\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th5GSqlFCK8I",
        "outputId": "4698a41d-b75f-445a-8fe7-61b82f35a78e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A. The product is too expensive.\n"
          ]
        }
      ],
      "source": [
        "sequences = generator(\n",
        "    prompt,\n",
        "    generation_config=generation_config,\n",
        "    pad_token_id=generator.tokenizer.eos_token_id,  # suppress \"Setting ...\"\n",
        ")\n",
        "for seq in sequences:\n",
        "    response = seq[\"generated_text\"].removeprefix(prompt)\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9McRvcoIr7Y"
      },
      "source": [
        "Wrong. You can try setting `do_sample=True` in the `GenerationConfig` but it rarely\n",
        "gives correct and consistently parseable results."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
