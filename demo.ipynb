{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**: demonstrates that the zero-shot text classification method [described here](https://stats.stackexchange.com/q/601159/337906) works ok. Currently just evaluates the method on the [COPA task](https://people.ict.usc.edu/~gordon/copa.html), since it's one of the [SuperGLUE tasks](https://super.gluebenchmark.com/tasks) in which labels have multiple tokens, in some sense.\n",
    "\n",
    "**Estimated run time**: ~1 min.\n",
    "\n",
    "**Environment**: See [`requirements.txt`](https://github.com/kddubey/lm-classification/blob/main/requirements.txt).\n",
    "\n",
    "**Other**: You have to have an OpenAI API key stored in the environment variable `OPENAI_API_KEY`. [Sign up here](https://openai.com/api/). This notebook will warn you about cost before incurring any. It'll cost ya about <span>$</span>0.50."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Load data](#load-data)\n",
    "\n",
    "[Write prompt](#write-prompt)\n",
    "\n",
    "[Run model](#run-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "import datasets as nlp_datasets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "from lm_classification import classify\n",
    "from lm_classification.utils import gpt2_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## When hitting the OpenAI endpoints, we'll log any server errors\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    handlers=[logging.StreamHandler(stream=sys.stdout)],\n",
    "                    format='%(asctime)s :: %(name)s :: %(levelname)s :: '\n",
    "                           '%(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this MVP, let's evaluate on the [Choice of Plausible Alternatives (COPA) task](https://people.ict.usc.edu/~gordon/copa.html). I picked this first b/c I read it has multi-token labels, in some sense. It also looks cool. The [SuperGLUE tasks](https://super.gluebenchmark.com/tasks) in general are pretty cool.\n",
    "\n",
    "The classification problem is to pick 1 of 2 alternatives which caused or resulted in the premise. Here are two example pulled from the website:\n",
    "\n",
    "Example 1\n",
    "\n",
    "> Premise: The man broke his toe. What was the CAUSE of this?\n",
    ">\n",
    "> Alternative 1: He got a hole in his sock.\n",
    ">\n",
    "> Alternative 2: He dropped a hammer on his foot.\n",
    "\n",
    "\n",
    "Example 2\n",
    "\n",
    "> Premise: I tipped the bottle. What happened as a RESULT?\n",
    ">\n",
    "> Alternative 1: The liquid in the bottle froze.\n",
    ">\n",
    "> Alternative 2: The liquid in the bottle poured out.\n",
    "\n",
    "A classifier should predict Alternative 2 for Example 1, and Alternative 2 for Example 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set labels are hidden, so I'll score this zero-shot classifier on the train and validation sets. I promise you I didn't tune anything on this data. There ain't much to tune to begin with :-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-21 19:53:09,414 :: datasets.builder :: WARNING :: Found cached dataset super_glue (C:/Users/kushd/.cache/huggingface/datasets/super_glue/copa/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n",
      "2023-02-21 19:53:11,843 :: datasets.builder :: WARNING :: Found cached dataset super_glue (C:/Users/kushd/.cache/huggingface/datasets/super_glue/copa/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    }
   ],
   "source": [
    "def load_super_glue(task_id: str, split: str):\n",
    "    return pd.DataFrame(nlp_datasets\n",
    "                        .load_dataset('super_glue', task_id, split=split))\n",
    "\n",
    "\n",
    "copa_df = (pd.concat((load_super_glue('copa', 'train'),\n",
    "                      load_super_glue('copa', 'validation')))\n",
    "           .reset_index(drop=True)) ## the idx column is only unique w/in splits! fuhgetaboutit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(copa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>choice1</th>\n",
       "      <th>choice2</th>\n",
       "      <th>question</th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My body cast a shadow over the grass.</td>\n",
       "      <td>The sun was rising.</td>\n",
       "      <td>The grass was cut.</td>\n",
       "      <td>cause</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The woman tolerated her friend's difficult beh...</td>\n",
       "      <td>The woman knew her friend was going through a ...</td>\n",
       "      <td>The woman felt that her friend took advantage ...</td>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The women met for coffee.</td>\n",
       "      <td>The cafe reopened in a new location.</td>\n",
       "      <td>They wanted to catch up with each other.</td>\n",
       "      <td>cause</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The runner wore shorts.</td>\n",
       "      <td>The forecast predicted high temperatures.</td>\n",
       "      <td>She planned to run along the beach.</td>\n",
       "      <td>cause</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The guests of the party hid behind the couch.</td>\n",
       "      <td>It was a surprise party.</td>\n",
       "      <td>It was a birthday party.</td>\n",
       "      <td>cause</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0              My body cast a shadow over the grass.   \n",
       "1  The woman tolerated her friend's difficult beh...   \n",
       "2                          The women met for coffee.   \n",
       "3                            The runner wore shorts.   \n",
       "4      The guests of the party hid behind the couch.   \n",
       "\n",
       "                                             choice1  \\\n",
       "0                                The sun was rising.   \n",
       "1  The woman knew her friend was going through a ...   \n",
       "2               The cafe reopened in a new location.   \n",
       "3          The forecast predicted high temperatures.   \n",
       "4                           It was a surprise party.   \n",
       "\n",
       "                                             choice2 question  idx  label  \n",
       "0                                 The grass was cut.    cause    0      0  \n",
       "1  The woman felt that her friend took advantage ...    cause    1      0  \n",
       "2           They wanted to catch up with each other.    cause    2      1  \n",
       "3                She planned to run along the beach.    cause    3      0  \n",
       "4                           It was a birthday party.    cause    4      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copa_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to model COPA is to prompt an LM with (for Example 1):\n",
    "\n",
    "```\n",
    "The man broke his toe. What was the cause of this?\n",
    "```\n",
    "\n",
    "and use the LM to estimate the probabilities of the 2 alternatives conditional on this prompt. (See the **Example** section [here](https://stats.stackexchange.com/q/601159/337906) for a full description of what \"compute the probabilities\" actually means. Or, check out the code in [`lm_classification.classify.predict_proba_examples`](https://github.com/kddubey/lm-classification/blob/main/lm_classification/classify.py#L219).)\n",
    "\n",
    "A potential issue with this approach is that any 2 alternatives are gonna have really low probabilities. As a result, discriminating between alternatives may be, numerically and statistically, a bad idea. But hey, that's why this notebook is here: let's see if this issue really does tank accuracy. And even if it does, we could always provide the alternatives in the prompt, as would be done w/ a sampling approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_copa(premise: str, question: str):\n",
    "    if question == 'cause':\n",
    "        question_ = 'What was the cause of this?'\n",
    "    elif question == 'effect':\n",
    "        question_ = 'What happened as a result?'\n",
    "    else:\n",
    "        raise ValueError( \"question must be 'cause' or 'effect'. Got \"\n",
    "                         f'{question}.')\n",
    "    return (f'{premise} {question_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "copa_df['prompt'] = [prompt_copa(premise, question)\n",
    "                     for premise, question\n",
    "                     in zip(copa_df['premise'], copa_df['question'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ac837_row0_col0, #T_ac837_row0_col1, #T_ac837_row0_col2, #T_ac837_row0_col3, #T_ac837_row1_col0, #T_ac837_row1_col1, #T_ac837_row1_col2, #T_ac837_row1_col3, #T_ac837_row2_col0, #T_ac837_row2_col1, #T_ac837_row2_col2, #T_ac837_row2_col3 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ac837_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >prompt</th>\n",
       "      <th class=\"col_heading level0 col1\" >choice1</th>\n",
       "      <th class=\"col_heading level0 col2\" >choice2</th>\n",
       "      <th class=\"col_heading level0 col3\" >label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ac837_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ac837_row0_col0\" class=\"data row0 col0\" >My body cast a shadow over the grass. What was the cause of this?</td>\n",
       "      <td id=\"T_ac837_row0_col1\" class=\"data row0 col1\" >The sun was rising.</td>\n",
       "      <td id=\"T_ac837_row0_col2\" class=\"data row0 col2\" >The grass was cut.</td>\n",
       "      <td id=\"T_ac837_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ac837_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_ac837_row1_col0\" class=\"data row1 col0\" >The woman tolerated her friend's difficult behavior. What was the cause of this?</td>\n",
       "      <td id=\"T_ac837_row1_col1\" class=\"data row1 col1\" >The woman knew her friend was going through a hard time.</td>\n",
       "      <td id=\"T_ac837_row1_col2\" class=\"data row1 col2\" >The woman felt that her friend took advantage of her kindness.</td>\n",
       "      <td id=\"T_ac837_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ac837_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_ac837_row2_col0\" class=\"data row2 col0\" >The women met for coffee. What was the cause of this?</td>\n",
       "      <td id=\"T_ac837_row2_col1\" class=\"data row2 col1\" >The cafe reopened in a new location.</td>\n",
       "      <td id=\"T_ac837_row2_col2\" class=\"data row2 col2\" >They wanted to catch up with each other.</td>\n",
       "      <td id=\"T_ac837_row2_col3\" class=\"data row2 col3\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b72b901ee0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_num_examples_displayed = 3\n",
    "with pd.option_context('max_colwidth', -1):\n",
    "    display(copa_df\n",
    "            [['prompt', 'choice1', 'choice2', 'label']]\n",
    "            .head(_num_examples_displayed)\n",
    "            .style ## render the newlines in the printed output\n",
    "            .set_properties(**{'text-align': 'left',\n",
    "                               'white-space': 'pre-wrap'}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for many SuperGLUE datasets, including COPA, the probability distribution over classes (alternative 1, 2 for COPA) is uniform. So we'll use `prior=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "copa_examples = [classify.Example(prompt=record['prompt'],\n",
    "                                  completions=(record['choice1'],\n",
    "                                               record['choice2']),\n",
    "                                  prior=None)\n",
    "                 for record in copa_df.to_dict('records')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(copa_examples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell warns you about price, and asks if you're ready to pay.\n",
    "\n",
    "(small TODO: I forgot what `input` does in Jupyter notebooks. I use VS Code notebooks now, and it just asks you to press the Enter key to proceed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = gpt2_tokenizer([example.prompt + classify.END_OF_PROMPT + completion\n",
    "                             for example in copa_examples\n",
    "                             for completion in example.completions])\n",
    "num_tokens = sum(len(tokens) for tokens in all_tokens['input_ids'])\n",
    "cost_per_1k_tokens = 0.02 ## https://openai.com/api/pricing/\n",
    "cost = round(num_tokens * cost_per_1k_tokens / 1_000, 2)\n",
    "\n",
    "output = input(f'The next cell will cost you ${cost}. Proceed?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing probs: 100%|██████████| 1000/1000 [00:24<00:00, 41.44it/s]\n"
     ]
    }
   ],
   "source": [
    "## 500 examples * 2 classes = 1000 OpenAI API requests\n",
    "pred_probs = classify.predict_proba_examples(copa_examples,\n",
    "                                             model='text-davinci-003')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For COPA, the scoring metric is accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.854"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_classes = pred_probs.argmax(axis=1)\n",
    "(pred_classes == copa_df['label']).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [leaderboards](https://super.gluebenchmark.com/submission/MoBKq4x8olYosIAnGvgF3NrRrpl2/-M8-O2ZNnHOirgs_EsH4), a few-shot sampling approach (\"priming\" GPT-3 w/ 32 training examples) gets 0.92 on the test set. So this method does ok."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18983e9e827802b63835b8dbc18d4f8a4ebd5dd283f63362764c2c714492b89c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
